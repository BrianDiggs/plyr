\documentclass[letterpaper,oneside]{scrartcl}
\usepackage{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.png,.pdf}
\usepackage{hyperref}
\usepackage{color}
\definecolor{slateblue}{rgb}{0.07,0.07,0.488}
\hypersetup{colorlinks=true,linkcolor=slateblue,anchorcolor=slateblue,citecolor=slateblue,filecolor=slateblue,urlcolor=slateblue,bookmarksnumbered=true,pdfview=FitB}
\usepackage{url}
\usepackage[round,sectionbib]{natbib}
\bibliographystyle{abbrvnat}
\usepackage[small]{caption2}
\usepackage[small]{titlesec}
\renewcommand\familydefault{bch}
\usepackage{slashbox}

\title{plyr: divide and conquer}
\author{Hadley Wickham}
\begin{document}
\maketitle

% \begin{abstract}
% plyr is a set of tools that solves a common set of problems: you need to break a big problem down into manageable pieces, operate on each pieces and then put all the pieces back together.  This paper describes the components that make up plyr.  It includes two case studies.
% \end{abstract}

\section{Introduction}

The {\tt plyr} package provides tools for solving a common class of problems, where you break apart a big data structure, operate on each piece independently and then put all the pieces back together (possibly in a different format to the original).  This paper introduces the {\tt ply} family of functions which generalise the {\tt apply} family, and include all combinations of input from and output to lists, data frames and arrays.

This paper describes version 0.1 of the package.  {\tt plyr} has no run-time dependencies, and requires R 2.7.0 or later.  To install it from within R, run {\tt install.packages("plyr")}.  Information about the latest version of the package, including updated version of this document, can be found online at  \url{http://had.co.nz/plyr}.  If you have any questions or comments about {\tt plyr}, please feel free to email me directly at \href{mailto:h.wickham@gmail.com}{h.wickham@gmail.com}.

In general, these tools provide a replacement for {\tt for} loops for a large set of practical problems.  The major assumption that they make is that each piece can be operated on independently, so if there is any dependence (e.g. recursive relationship) between the pieces then these tools are not appropriate.  It is not true that for loops are slow, but often they do not clearly express the intent of your algorithm, as you need a lot of extra housekeeping code. The tools of {\tt plyr} aim to eliminate this extra code and illuminate the key components of your computations.

 The {\tt plyr} package also provides a number of helper functions for error recovery, splatting, column-wise processing, and reporting progress. These are described in Section~\ref{sec:helpers}. Section~\ref{sec:strategy} discusses the general strategy that these functions support, including cases studies that explore long term professional baseball players and ozone measured over space and time.  Finally, Section~\ref{sec:equiv} maps existing R functions to their plyr counterparts and lists related packages, and Section~\ref{sec:future} describes future plans.

% Note that through this paper we will use array to refer to vectors (1d arrays) and matrices (2d arrays) as well.  

\section{Usage}

Table~\ref{tbl:functions} lists the basic set of plyr functions.  Each function is named according to the type of input it processes and the type of output it produces.  The type of the input determines how it can be broken up, and the various possibilities described in detail in Section~\ref{sec:input}.  The output type determines how the pieces are joined back together again, as described in detail in Section~\ref{sec:output}. 

\begin{table}
  \begin{center}
  \begin{tabular}{l|ccc}
    
     \backslashbox{{\bf to}}{{\bf from}} & array & data.frame & list \\
     \hline
     array      & aaply  & daply  & laply  \\
     data.frame & adply  & ddply  & ldply  \\
     list       & alply  & dlply  & llply  \\
     nothing    & a\_ply & d\_ply & l\_ply \\
    
  \end{tabular}
  \end{center}
  \caption{The 12 key functions that make up {\tt plyr}.  Arrays include matrices and vectors as special cases.}
  \label{tbl:functions}
\end{table}

Arguments to the ply functions are determined by the types of input and output.  For this reason, it's useful to refer to a complete row or column of Table~\ref{tbl:functions}.  The notation we use for this is {\tt d*ply} to refer an entire row (fixed input) and {\tt *dply} for an entire column (fixed output).

The first argument, {\tt data}, for all {\tt ply} functions is the data, a list, data frame or array.  The second argument describes how to split up the data, and is different for each input type.  The third argument is the function to be applied to each piece.

\subsection{Input} 
\label{sec:input}

\begin{itemize}
  \item Arrays are sliced 
  
  \item Data frames are split into groups based on combinations of variables
  
  \item Lists are assumed to be broken up already, and so this argument is omitted
  
\end{itemize}

\subsubsection{In: array ({\tt a*ply})}

The {\tt margins} argument of {\tt a*ply} describes how to slice up the array in the same way that {\tt apply} does.  For example, {\tt margins = 1} specifies that we want to break up the array by rows (the first index when subsetting), and {\tt margins = 2} by columns (the second index when subsetting).  You can also use combinations of margins.  For example, {\tt margins = 1:2} will split up by the first two dimensions.  For a 3d array, this will produce the columns in the z-direction.

A special case of operating on arrays corresponds to the {\tt mapply} function of base R.  The plyr equivalents are named {\tt maply}, {\tt mdply}, {\tt mlply} and {\tt m\_ply}.  These default to working on the first dimension (i.e. row-wise) and automatically splat the function so that function is called not with a single list as input, but each column is passed as a separate argument to the function.  Compared to using {\tt mapply}, for the {\tt m*ply} functions you will need to {\tt cbind} the columns together first.  This will ensure that each argument has the same length, and allows the {\tt m*ply} functions to have the same argument order as all the other 

Can pass data frame if want to treat as 2d structure.  

\subsubsection{In: data frame ({\tt d*ply})}

When operating on a data frame, you usually want to split it up into groups based on combinations variables in the data set.  For {\tt d*ply} you specify   which variables (or functions of variables) to use.  These variables are specified in a special way to highlight that they are computed first from the data frame, then the global environment (in which case it's your responsibility to ensure that their length is equal to the number of rows in the data frame).  

\begin{itemize}
  \item The interaction of multiple variables are taken: {\tt .(a, b, c)}
  \item Functions of variables: {\tt .(round(a))}, {\tt .(a * b)}
  \item Variables in the global environment {\tt .(anothervar)}
\end{itemize}

\subsection{Output}
\label{sec:output}

The output type defines how the pieces will be joined back together again, and how they will be labelled.  The labels are particularly important to allow you to match up the input with the output.

The input and output types are the same, except there is an additional output option, which discard the output.  This is useful for functions with side effects that make changes outside of R

The output type also places some restrictions on what type of results the processing function should return.  Generally, the processing function should return the same type of data as the eventual output, (i.e.\ vectors, matrices and arrays for {\tt *aply} and data frames for {\tt *dply}) but some other formats are accepted for convenience and are described below.  

% For any type, if the processing function returns {\tt NULL}, that slice will not be included in the output.

\subsubsection{Out: array ({\tt *aply})}

With array output the dimensionality is determined by the input splits.  A list will produce a single dimension, a data frame will have a dimension for each variable split on, and a array will have a dimension for each dimension that it was split on.  The processing function should return an atomic (i.e.\ {\tt is.atomic(x) == TRUE}) of array of fixed size/shape, or a list.  If atomic, the extra dimensions will added perpendicular to the original dimensions.  If a list, the output will be a list with dimensions.  

Some examples should make this easier to understand:

The dimnames of the array will be the same as the input, if an array, or the extracted from the subsets if a data frame.

If there are no results, {\tt adply} will return a logical vector of length 0.

\subsubsection{Out: data frames ({\tt *dply})}

The processing functions should either return a data.frame, or a (named) atomic vector of fixed length, which will form the columns of the output.

If there are no results, {\tt *dply} will return an empty data frame.

The output data frame will be supplemented with columns that identify the subset of the original dataset that each piece was computed from.  These columns make it easier to merge the old and new data.  If the input was a data frame, this will be the values of the splitting variables.  If the input was an array, this will be the dimension names.

\subsubsection{Out: list ({\tt *lply})}

This is the simplest output format, where each processed piece is joined together in a list.  The list also stores the labels associated with each pieces, so that if you use {\tt ldply} or {\tt laply} to further process the list the labels will appear as if you had used {\tt aaply}, {\tt adply}, {\tt daply} or {\tt ddply} directly.  {\tt llply} is convenient for calculating complex objects once (e.g.\ models), from which pieces of interest are later extracted into arrays and data frames.

There are no restrictions on the output of the processing function.  If there are no results, {\tt *lply} will return a list of length 0.

\subsection{Out: nothing ({\tt *\_ply})}

Sometimes you are operating on a list purely for the side effects (e.g. plots, caching, output to screen/file).  This is a little more efficient than abandoning the output of {\tt *lply} because it doesn't store the intermediate results.

\section{Helpers}
\label{sec:helpers}

The {\tt plyr} package also provides a number of helper function which take a function (or functions) as input and return a modified function as output.  

\begin{itemize}
  \item {\tt splat} converts a function to use.  This is useful when you want to pass a function a row of data frame or array, and don't want to manually pull it apart in your function.  For example:
  
  \begin{verbatim}
    hp_per_cyl <- function(hp, cyl, ...) hp / cyl
    splat(hp_per_cyl)(mtcars[1,])
    splat(hp_per_cyl)(mtcars)
  \end{verbatim} 
  
  Generally, splatted functions should have {\tt ...} as an argument, so you only need to specify the variables that you are interested in.  For more information on how splat works, see {\tt do.call}.  
  
  {\tt splat} is applied to functions used in {\tt m*ply} by default.

  \item {\tt each} takes a list of functions and produces a function that runs  each function on the inputs and returns a named vector of outputs.   For example, {\tt each(min, max)} is short hand for {\tt function(x) c(min = min(x), max = max(x))}.  Using each with a single function is useful if you want a named vector as output.

  \item {\tt colwise} converts a function that work on vectors, to one that operates column-wise of data frame, returning a vector of results named according to the columns of the data frame.  For example, {\tt colwise(median)} is a function that computes the median of each column of a data.frame.  
  
  The optional {\tt .if} argument specialises the function to only run on certain types of vector, e.g.\ {\tt .if = is.factor} or {\tt .if = is.numeric}.  These two restrictions are provided in the premade {\tt calcolwise} and {\tt numcolwise}.  
  
  If the additional argument {\tt .try} is {\tt TRUE}, then the function will be wrapped with {\tt failwith(NA, f, quiet = .quiet)}. 
  
  \item {\tt failwith} sets a default value to return if the function throws an error.  For example, {\tt failwith(NA, f)} will return an {\tt NA} whenever {\tt f} throws an error.  
  
  The optional {\tt quiet} argument suppresses any notice of the error when it is {\tt TRUE}.

\end{itemize}

Each plyr function also has a {\tt .progress} argument which allows you to monitor the progress of long running operations.  There are four difference progress bars:

\begin{itemize}
  \item {\tt "none"}, the default.  No progress bar is displayed.
  \item {\tt "text"} provides a textual progress bar which.
  \item {\tt "win"} and {\tt "tk"} provide graphical progress bars for Windows and systems with the tcl/tk package loaded.
\end{itemize}

The progress bars assume that processing each piece takes the same amount of time, so will not be 100\% accurate.

\section{Strategy}
\label{sec:strategy}



\begin{enumerate}
  \item Extract a subset of the data for which it is easy to solve the problem
  \item Solve the problem by hand, checking as you go
  \item Write a function that encapsulates the solution
  \item Use the appropriate ply function to split up the original data, apply the function and join the pieces back together.
  
\end{enumerate}

The following two case studies illustrate these techniques for a range of problems related to a data frame storing the batting records for long-term baseball players, and a 3d array representing space and time values of ozone.

\subsection{Case study: baseball data}

The {\tt baseball} data set contains the batting records for all professional US players with 15 or more years of data.  The complete list of variables are described fully {\tt ?baseball}, but for this example we will focus on just four: {\tt id}, which identifies the player, {\tt year} the year of the record and {\tt rbi} the number of runs that the player made in the season, and {\tt} at bat, the number of times the player had an opportunity to hit the ball.

(This is a rather crude analysis, as it doesn't take into account the people that might already be on the other plates)

What we'll explore is the performance of a batter over his career.  To get started, we need to calculate the careeryear, i.e. the number of years since the player started playing.  This is easy to do if we have a single player:

\begin{verbatim}
baberuth <- subset(baseball, id == "ruthba01")
baberuth$cyear <- baberuth$year - min(baberuth$year) + 1
\end{verbatim}

To do this for all players, we first make a function:

\begin{verbatim}
calculate_cyear <- function(df) {
  transform(df, 
    cyear = year - min(year),
    cpercent = (year - min(year)) / (max(year) - min(year))
  )
}
\end{verbatim}

\noindent and then split up the whole data frame into people, run the function on each piece and join them back together into a data frame:

% qplot(ab, data=baseball, geom="histogram", binwidth=10)
% qplot(ab, data=subset(baseball, ab < 100), geom="histogram", binwidth=5)

\begin{verbatim}
baseball <- ddply(baseball, .(id), calculate_cyear)
baseball <- subset(baseball, ab >= 25)
\end{verbatim}

% qplot(cyear, rbi / ab, data=baseball, group=id, geom="line", colour=I(alpha("black", 1/20)))

To summarise the pattern across all players, we first need to figure out what the common patterns are.  A time series plot of rbi/ab is a good place to start.  We figure out how do this for Babe Ruth (shown), then write a function, and then use {\tt d\_ply} to save a plot for every player to a pdf.

\begin{verbatim}
  xlim <- range(baseball$cyear, na.rm=TRUE)
  ylim <- range(baseball$rbi / baseball$ab, na.rm=TRUE)
  plotpattern <- function(df) {
    print(qplot(cyear, rbi / ab, data = df, geom="line", xlim = xlim, ylim = ylim ))
  }
  
  pdf("paths.pdf", width=8, height=4)
  d_ply(baseball, .(reorder(id, rbi / ab)), failwith(NA, plotpattern))
  dev.off()
\end{verbatim}

Show Babe Ruth.  

Flicking through the 1145 plots reveals that there doesn't seem to be much of a common pattern, although many players do seem to have a roughly linear trend with quite a bit of noise.  We'll start by fitting a linear model to each player and then exploring the results.  This time we'll skip doing it by hand and go directly to the function.  This is probably ok here, since it's a simple procedure.

\begin{verbatim}
model <- function(df) {
  lm(rbi / ab ~ cyear, data=df)
}
model(baberuth)
models <- dlply(baseball, .(id), model)
\end{verbatim}

Now we have a list of 1145 models, one for each player.  To do something interesting with these, we need to extract some summary statistics.  We'll extract the coefficients of the model (the slope and intercept).  It's also important to get some measure of model fit so we can ensure we're not drawing conclusions based on models that fit the data very poorly.  If we were going to go into more depth, it would be a good idea to look at the players that the model fits very poorly and come up with some better summaries.

\begin{verbatim}
rsq <- function(x) summary(x)$r.squared
coef <- ldply(models, function(x) c(coef(x), rsq(x)))
names(coef) <- c("id", "intercept", "slope", "rsquare")
\end{verbatim}

Plots

\begin{verbatim}
  ggplot(coef, aes(slope, intercept)) + scale_size() + geom_vline(size=0.5, colour="grey50") + geom_hline(size = 0.5, colour="grey50") + geom_point(aes(size = rsquare))
  last_plot() + xlim(-0.01, 0.01) + ylim(-0.1, 0.25)
\end{verbatim}

Reassuringly, there are no players in the bottom left quadrant with both negative slope and intercept

% model2 <- function(df) {
%   lm(rbi / ab ~ cyear + I(cyear ^ 2), data=df)
% }
% models2 <- dlply(baseball, .(id), model2)
% coef <- ldply(models2, function(x) c(coef(x), rsq(x)))
% names(coef) <- c("id", "intercept", "a", "b", "rsquare")
% coef$peak <- with(coef, -a / (2 * b))
% 
% qplot(peak, data=coef, geom="histogram")
% qplot(peak, data=subset(coef, peak > 0 & peak < 20), geom="histogram", binwidth=1)
% qplot(peak, data=subset(coef, rsquare > 0.8), geom="histogram", binwidth=1)
% 
% qplot(a, b, data=coef, size=rsquare) + scale_size()
% 

\subsection{Case study: spatio-temporal ozone distribution}

Standardisation/smoothing.

For many other types of operations, it is useful to convert this array structure to a data frame.  The {\tt melt} function in the {\tt reshape} package is one way to do that which preserves the dimension labels as much as possible.

\section{Equivalence to existing R functions}
\label{sec:equiv}

Table~\ref{tbl:equiv} describes the equivalent between functions in base R and 
the functions provided by {\tt plyr}.  The built in R functions focus mainly on arrays and lists, not data frames, and most provide an argument to determine whether an array or list should be returned.  The syntax is also less consistent than plyr, for example, {\tt mapply} takes a function as the first argument rather than the input data.  Compared to {\tt apply}, {\tt aaply} returns the dimensions in a different order so as to be idempotent - i.e.\ {\tt apply(x, a, function(x) x) == x} for all {\tt a}. 

Avoid any ambiguity about what you'll get back from one of these functions. This replaces the {\tt simplify} argument that many of the {\tt apply} functions in base R has, and means that you can depend on the output of each function being a given type (which makes programming with the results easier).


\begin{table}[htpb]
  \begin{center}
  \begin{tabular}{llll}
    base & from & to & plyr \\
    \hline
    apply     & a & a   & aaply \\
    lapply    & l & l   & llply \\
    sapply    & l & a   & laply \\
    mapply    & a & a/l & maply / mlply \\
    by        & d & l   & dlply \\
    aggregate & d & d   & ddply + colwise \\
  \end{tabular}
  \end{center}
  \caption{Mapping between apply functions and plyr functions.}
  \label{tbl:equiv}
\end{table}

Related functions {\tt tapply}, {\tt ave} and {\tt sweep} have no corresponding function in {\tt plyr}, and still remain useful. {\tt merge} is also for combining summaries with the original data.  The cast function in the reshape package \citep{reshape} is closely related to {\tt aaply}.

There are a number of other resources that also attempt to simplify this class of problems:

\begin{itemize}
  \item Base R:  apply functions, by, etc.  The equivalence between these function and plyr functions are described in Section~\ref{sec:equiv}.
  
  \item The {\tt doBy} package
  \item The {\tt gdata} package
  \item The {\tt scope} package
  \item Data manipulation in R, by Phil Spector
  \item Chapters in MASS, R intro?
  
\end{itemize}

\section{Future plans}
\label{sec:future}

If slow, might want to look at the profr package to speed up.  

However, it is my aim to eventually implement these functions in C for maximum speed and memory efficiency, so that they are competitive with the built in operations.  I also plan to investigate a connection to the {\tt papply} function to allow for easy parallelisation across multiple instances of R (particularly for multi-core machines).

\bibliography{/Users/hadley/documents/phd/references}
\end{document}
